{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99402b96",
   "metadata": {},
   "source": [
    "# Multi Stream Live Chat Analysis Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f27cbe6",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Looking at the dataset we found out that the sentiment of the messages is:\n",
    "   * $4$ to identify a message with positive sentiment\n",
    "   * $0$ to identify a message with negative sentiment\n",
    "\n",
    "Then the first preprocessing step is to change all positive messages sentiment from $4$ to $1$.\n",
    "\n",
    "Now speaking about the text data we have that since the dataset is composed by raw data from Twitter, each Twit must be preprocessed to be usable.\n",
    "\n",
    "The pipeline used to preprocess the data is the following:\n",
    "   1. Transform all the text in lower case\n",
    "   2. Substitute emojis into text\n",
    "   3. Explicit negations\n",
    "   4. Replace any url with \"URL\"\n",
    "   5. Replace user tags with \"USR\"\n",
    "   6. Replace $3$ consecutive chars with $2$\n",
    "   7. Remove consecutive spaces\n",
    "   8. Remove hashtags\n",
    "   9. Stemming\n",
    "   \n",
    "###### Lower Case\n",
    "\n",
    "This is applied in order to don't distinguish between lower case and upper case words. In previous years people tend to enfatize expressions user all upper case sentences, but nowdays this is not a thing anymore and people just use emojis or a more direct text\n",
    "\n",
    "###### Emojis\n",
    "\n",
    "Here we will replace the most common emojis into their explicit form (i.e. ':)' will become 'smile'). This is used in order to make everything of the same form and reduce the complexity of models and word embeddings.\n",
    "\n",
    "Also must be noticed that Twitch emojis are already in the explicit form.\n",
    "\n",
    "###### Negations\n",
    "\n",
    "Here we will replace the most common negations into an explicit format (i.e. 'haven't' will become 'have not'). This is used in order to reduce the complexity of models and word embeddings.\n",
    "\n",
    "###### URLs\n",
    "\n",
    "Here we will replace each url into the string \"URL\" since trying to evaluate a message by its url without analyzing the content of the url can be misleading, so we will just point out that there is an url.\n",
    "\n",
    "###### User Tags\n",
    "\n",
    "Here we will replace each url into the string \"URL\" since trying to evaluate a message by its url without analyzing the content of the url can be misleading, so we will just point out that there is an url.\n",
    "\n",
    "###### Remove Hashtags\n",
    "\n",
    "Hashtags are not a thing in Twitch, so it is better to remove them in order to have data more related to our case.\n",
    "\n",
    "###### Stemming\n",
    "\n",
    "The last step in our preprocessing pipeline is stemming.\n",
    "\n",
    "Stemming is a technique used to extract the base form of the words by removing affixes from them. For example, the stem of the words eating, eats, eaten is eat.\n",
    "\n",
    "But since this is a naive algorithms it may happen that a word is truncated into another word which has no meaning whatsoever.\n",
    "\n",
    "The stronger version of stemming is Lemming, which does the same thing of stemming but with the constraint that the final word has an actual meaning and is the real root form of the original word.\n",
    "\n",
    "Intuitively it would make more sense to apply lemming in order to don't generate meaningless words, but actually lemming is way heavier (computationally speaking), so we decided to use a lightweight procedure.\n",
    "\n",
    "###### Implementation\n",
    "\n",
    "Here we define the class that we will use to preprocess our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86572b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess():\n",
    "    def __init__(self, negations, emojis, regex_subs, stemmer):\n",
    "        self.negations = negations\n",
    "        self.emojis = emojis\n",
    "        self.regex_subs = regex_subs\n",
    "        self.stemmer = stemmer\n",
    "    \n",
    "    def sub_emoji(self, text):\n",
    "        for emoji in self.emojis.keys():\n",
    "            text = text.replace(emoji, self.emojis[emoji]) \n",
    "        return text\n",
    "\n",
    "    def sub_negations(self, text):\n",
    "        for negation in self.negations.keys():\n",
    "            text = text.replace(negation, self.negations[negation])\n",
    "        return text\n",
    "\n",
    "    def sub_regexs(self, text):\n",
    "        for regex in self.regex_subs.keys():\n",
    "            text = re.sub(regex, self.regex_subs[regex], text)\n",
    "        return text\n",
    "\n",
    "    def stemming(self, text):\n",
    "        stemmed_text = \"\"\n",
    "        for word in text.split():\n",
    "            word = self.stemmer.stem(word)\n",
    "            stemmed_text += (word + \" \")\n",
    "        return stemmed_text\n",
    "\n",
    "    def text_preprocess(self, text):\n",
    "        text = text.lower()\n",
    "        text = self.sub_emoji(text)\n",
    "        text = self.sub_negations(text)\n",
    "        text = self.sub_regexs(text)\n",
    "        text = self.stemming(text)\n",
    "        return text\n",
    "\n",
    "    def sentiment_preprocess(self, sentiment):\n",
    "        if sentiment == 4 :\n",
    "            sentiment = 1\n",
    "        return sentiment\n",
    "\n",
    "    def df_pre_process(self, df, var_text, var_sentiment):\n",
    "        print(\"starting preprocessing...\")\n",
    "        df[var_text] = df[var_text].apply(lambda x: self.text_preprocess(x))\n",
    "        df[var_sentiment] = df[var_sentiment].apply(lambda x: self.sentiment_preprocess(x))\n",
    "        print(\"...preprocessing completed\")\n",
    "        return df\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e90a7b",
   "metadata": {},
   "source": [
    "And here it is how we used this class in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "negations = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "\n",
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused',\n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "\n",
    "regex_subs = {r\"https?://[^s]+\" : \"URL\", # replace any url with URL\n",
    "              \"www.[^ ]+\" : \"URL\", # replace any url with URL\n",
    "              r\"@[^\\s]+\" : \"USR\", # replace any user tag with USR (the tag system is the same also in twitch)\n",
    "              r\"(.)\\1\\1+\" : r\"\\1\\1\", # replace 3 consecutive chars with 2\n",
    "              r\"[\\s]+\" : \" \", # remove consec spaces\n",
    "              \"#[a-z0-9]*\" : \"\" #remove hashtags, they are not used in twitch chats\n",
    "}\n",
    "\n",
    "sbStem = SnowballStemmer(\"english\", True)\n",
    "preprocess = ps.Preprocess(negations, emojis, regex_subs, sbStem)\n",
    "df = preprocess.df_pre_process(df, \"text\", \"sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d103a58",
   "metadata": {},
   "source": [
    "## Embedding Algorithms\n",
    "\n",
    "In order to process text we have to convert the raw input into a meaningful numeric format. This numeric format cannot consist of the naive transformation of each char into its corresponding ASCII value because this would lead to remove all the semantic in the text. Instead, a word embedding algorithm will be used in order to both make the input treatable by a machine learning model and to retain the semantic in the data.\n",
    "\n",
    "The ideal solution for this problem would be using the actual state-of-the art embedding model: the BERT embedding algorithm. But considering that the word embedding must be applied in a real time streaming of messages (potentially thousands of messages per second) and considering that this algorithm is actually a huge Transformer Deep Neural Network, this would be really time consuming and would lead to a pretty huge delay in the processing of the messages, loosing the real time property of this project.\n",
    "\n",
    "Here we will evaluate two embedding algorithms: TF-IDF and word2vec.\n",
    "\n",
    "In the following two sections we will explain the two embedding algorithms and define a function to use them in our pipeline.\n",
    "\n",
    "The actual choice of the embedding algorithm and the final parameters of the chosen embedding algorithm will be done during the Classification (sentiment analysis) process.\n",
    "\n",
    "#### TF-IDF\n",
    "\n",
    "TF-IDF, short for term frequencyâ€“inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n",
    "\n",
    "The TF-IDF value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "\n",
    "The TF term is defined as follows: $$TF(t, d) = \\frac{f_{t,d}}{\\sum_{t'\\in d}f_{t',d}}$$\n",
    "\n",
    "where $f_{t,d}$ is the raw count of the term $t$ in the document $d$.\n",
    "\n",
    "The IDF term is defined as follows: $$IDF(t,D)=\\log\\frac{N}{|\\{d\\in D:t\\in d\\}|}$$\n",
    "\n",
    "Where $N$ is the number of documents (in our case the number of messages in the training set), and $|\\{d\\in D:t\\in d\\}|$ is the number of documents (number of messages) where the term $t$ appears.\n",
    "\n",
    "TF-IDF is then calculated as follows: $$TF-IDF(t,d,D)=TF(t,d)\\cdot IDF(t,D)$$\n",
    "\n",
    "Must be noted that the word \"term\" has been used and not \"word\", because this algorithm can be applied not only to a single word but also to $n-$grams ($n$ consecutive words).\n",
    "\n",
    "Here there is the definition of the class we will use to implement $TF-IDF$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef71ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class myTFIDF:\n",
    "    def __init__(self, df, max_features=100, ngram_range=(1,2)):\n",
    "        self.max_features = max_features\n",
    "        self.ngram_range = ngram_range\n",
    "        self.vectorizer = self.get_tfidf_vectorizer(df)\n",
    "        \n",
    "    def get_tfidf_vectorizer(self, df):\n",
    "        vectoriser = TfidfVectorizer(ngram_range = self.ngram_range, max_features = self.max_features)\n",
    "        vectoriser.fit(df)\n",
    "        return vectoriser\n",
    "\n",
    "    def df_tfidf_vectorize(self, X):\n",
    "        print(\"starting vectorizing words...\")\n",
    "        #vectorizer = self.get_tfidf_vectorizer(X)\n",
    "        X = self.vectorizer.transform(X)\n",
    "        print(\"...words vectorized\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed399c",
   "metadata": {},
   "source": [
    "#### Word2Vec\n",
    "\n",
    "Word2Vec is a shallow, two-layer neural networks which is trained to reconstruct linguistic contexts of words. It takes as its input a large corpus of words and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space.\n",
    "\n",
    "Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.\n",
    "\n",
    "Word2Vec is a particularly computationally-efficient predictive model for learning word embeddings from raw text.\n",
    "\n",
    "It comes in two flavors, the Continuous Bag-of-Words (CBOW) model and the Skip-Gram model.\n",
    "\n",
    "###### Continuous Bag-of-Words (CBOW)\n",
    "\n",
    "CBOW predicts target words (e.g. â€˜matâ€™) from the surrounding context words (â€˜the cat sits on theâ€™). Statistically, it has the effect that CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets.\n",
    "\n",
    "###### Skip-Gram\n",
    "\n",
    "Skip-gram predicts surrounding context words from the target words (inverse of CBOW). Statistically, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have larger datasets.\n",
    "\n",
    "###### Architecture\n",
    "\n",
    "The architecture is similar to an autoencoderâ€™s one, you take a large input vector, compress it down to a smaller dense vector and then instead of decompressing it back to the original input vector as you do with autoencoders, you output probabilities of target words.\n",
    "\n",
    "First of all, we cannot feed a word as string into a neural network. Instead, we feed words as one-hot vectors, which is basically a vector of the same length as the vocabulary, filled with zeros except at the index that represents the word we want to represent, which is assigned \"$1$\".\n",
    "\n",
    "The hidden layer is a standard fully-connected (Dense) layer whose weights are the word embeddings.\n",
    "\n",
    "The output layer outputs probabilities for the target words from the vocabulary.\n",
    "\n",
    "###### Implementation\n",
    "\n",
    "Here we will define the functions that we will use to apply word2vec embedding to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab439a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "class myWord2Vec:\n",
    "    def __init__(self, corpus, vector_size, window, min_count, n_proc, epochs):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.n_proc = n_proc\n",
    "        self.epochs = epochs\n",
    "        self.model = self.get_w2v_vectorizer(corpus)\n",
    "        \n",
    "    def tokenize(self,df):\n",
    "        corpus = []\n",
    "        j = 0\n",
    "        for col in df:\n",
    "            word_list = col.split(\" \")\n",
    "            word_list = ' '.join(word_list).split()\n",
    "            tagged = TaggedDocument(word_list, [j])\n",
    "            j = j+1\n",
    "            corpus.append(tagged)\n",
    "        return corpus\n",
    "\n",
    "    def get_w2v_vectorizer(self, corpus):\n",
    "        corpus = self.tokenize(corpus)\n",
    "        model = Doc2Vec(vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=self.n_proc, epochs = self.epochs)\n",
    "        model.build_vocab(corpus)\n",
    "        model.train(corpus, total_examples=model.corpus_count\n",
    "                , epochs=model.epochs)\n",
    "        return model\n",
    "\n",
    "    def text_w2v_vectorize(self, text):\n",
    "        vectorized = self.model.infer_vector(text.split(' '))\n",
    "        return vectorized\n",
    "\n",
    "    def df_w2v_vectorize(self, df):\n",
    "        card2vec = [self.text_w2v_vectorize(df.iloc[i])\n",
    "                for i in range(0,len(df))]\n",
    "        return card2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc5484f",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "In this section the Sentiment Analysis part will be explored, starting from the algorithm Choices, their respective training, tuning and evaluations.\n",
    "\n",
    "Sentiment analysis is a technique through which you can analyze a piece of text to determine the sentiment behind it. It combines machine learning and natural language processing (NLP) to achieve this.\n",
    "\n",
    "Using basic Sentiment analysis, a program can understand whether the sentiment behind a piece of text is positive, negative, or neutral.\n",
    "\n",
    "#### Metric choice\n",
    "\n",
    "#### Algorithms choice\n",
    "\n",
    "In order to choose the sentiment analysis algorithms we have to identify what will be the important features that a model must have in order to be used in this context.\n",
    "\n",
    "Considering that we have to analize real time data where, potentially, thousands of messages per second can arrive our model must be computationally efficient, hence we have to avoid all the \"lazy algorithms\", like KNN, that don't need to be trained and evaluate the whole dataset whenever a new data entry have to be classified since this class of algorithm will be really time consuming. Also for the same reason we have to avoid all the ensemble learning algorithms, which combine several simple models to have a more robust model.\n",
    "\n",
    "Also for efficiency reasons we have to decide if using a more advanced text embedding algorithm or a non-linear classification algorithm, because a complex embedding algorithm can potentially produce thousands of features and feeding this to a non-linear algorithm can take several seconds to compute a single message. Instead if we use a simpler embedding algorithm, which will extract a very few number of features from the text, then we can try to implement a non-linear algorithm.\n",
    "\n",
    "Another element that will affect our models choice will be the interpretability of the model (remind that our final goal is to show the sentiment of the messages in a dashboard and so the user must be able to understand the meaning of the graph on the fly).\n",
    "\n",
    "Obvously the third element required for a model is the accuracy, our model must be able to correctly identify both positive and negative comments.\n",
    "\n",
    "Considering the three factors above, we will try to evaluate the following algorithms:\n",
    "   1. Naive Bayes (Gaussian, Multinomial and Bernoulli)\n",
    "   2. Decision Trees\n",
    "   3. SVM\n",
    "   4. Logistic Reression\n",
    "   \n",
    "In the following sections we will analyze all the models listed above and, if they seem to be usable in our context we will train them in a relatively small portion of our dataset in order to find the best combinations of embedding-classification algorithms and then we will fine tune the ones with the best results in order to try to make a strong but fast model to identify the sentiment of our messages.\n",
    "\n",
    "The following function will be used to resize the original dataset in such a way to mantain the balance on the two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34757eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(dataset, n_obs, var_check, value):\n",
    "    half = math.floor(n_obs/2)\n",
    "    data_pos = dataset[dataset[var_check] == value]\n",
    "    data_neg = dataset[dataset[var_check] != value]\n",
    "    data_pos = data_pos.iloc[random.sample(range(0, data_pos.count()[0]+1), half)]\n",
    "    data_neg = data_neg.iloc[random.sample(range(0, data_neg.count()[0]+1), half)]\n",
    "    dataset = pd.concat([data_pos, data_neg])\n",
    "    dataset = dataset.sample(frac=1)\n",
    "    dataset = dataset.reset_index(drop=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a2b9be",
   "metadata": {},
   "source": [
    "### Implementation Design\n",
    "\n",
    "In order to find the best candidates we will run a random search for the model hyperparameters and then we will run a grid search for the best model(s) in order to pick up the optimal model.\n",
    "\n",
    "Since also the embedding algorithm will affect the overall results of the models we have to fit also the random/grid search for the embedding algorithms into our decisional process.\n",
    "\n",
    "In order to do that we will create two child-classes for each model (one that will use TFIDF and the other one that will use word2vec), extending the models in the sklearn module, in order to allow the random/grid search algorithm to also try different configurations of the embedding algorithms.\n",
    "\n",
    "#### Naive Bayes\n",
    "\n",
    "Naive Bayes classifiers are a collection of classification algorithms based on Bayesâ€™ Theorem. It is not a single algorithm but a family of algorithms where all of them rely on the same assumption: each feature makes an independent and equal contribution to the outcome.\n",
    "\n",
    "Even though the assumption that all the features are uncorrelated is pretty naive this class of algorithms have been proved to often work fine in practice. Also this class of algorithms are really fast and allow to output the probability for a certain input to be part of a certain class, which in our case translates into the probability of a message to be positive and this can be directly interpreted as the positivitiness of a message (value $1$ means fully positive, value $0$ means fully negative and a value of $0.5$ means a neutral message since the algorithm cannot classify it in a perfect way).\n",
    "\n",
    "Before discussing the three main Naive Bayes algorithm we have to present the Bayes Theorem.\n",
    "\n",
    "###### Bayes Theorem\n",
    "\n",
    "Bayesâ€™ Theorem finds the probability of an event occurring given the probability of another event that has already occurred. Bayesâ€™ theorem is stated mathematically as the following equation: $$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$$ where $A$ and $B$ are events and $P(B)\\neq 0$ (basically, we are trying to find the probability of event $A$, given the result of event $B$)\n",
    "\n",
    "Event $B$ is also termed as evidence (The evidence is an attribute value of an unknown instance, here, it is event $B$).\n",
    "\n",
    "$P(A)$ is the priori of $A$ (the prior probability, i.e. Probability of event before evidence is seen).\n",
    "\n",
    "$P(A|B)$ is a posteriori probability of $B$, i.e. probability of event after evidence is seen.\n",
    "\n",
    "Applying it to our context we have that the event $A$ is the fact that a certain input falls into a certain class $y$, and the event $B$ is the fact that a certain feature $X$ takes a specific value.\n",
    "\n",
    "So the formula become: $$P(y|X)=\\frac{P(X|y)P(y)}{P(X)}$$\n",
    "\n",
    "Considering the naive assumption of the model we can extend the Bayes Theorem in having multipe evidence terms (in our case more features) and the theorem became: $$P(y|X_1,...,X_n)=\\frac{P(y)\\prod_{i=1}^n{P(X_i|y)}}{\\prod_{i=1}^n{P(X_i)}}$$\n",
    "\n",
    "###### Gaussian Naive Bayes Classifier\n",
    "\n",
    "In Gaussian Naive Bayes, continuous values associated with each feature are assumed to be distributed according to a Gaussian distribution.\n",
    "\n",
    "The likelihood of the features is assumed to be Gaussian, hence, conditional probability is given by: $$P(x_i|y)=\\frac{1}{\\sqrt{2\\pi\\sigma_y^2}}\\exp\\left( -\\frac{(x_i-\\mu_y)^2}{2\\sigma_y^2} \\right)$$\n",
    "\n",
    "Here it is the implementation of the Gaussian Naive Bayes model with TFIDF embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c5271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFGaussianNB(GaussianNB):\n",
    "    def __init__(self, *, tfidf_max_features=100, ngram_range=(1, 2), priors=None, var_smoothing=1e-9):\n",
    "        super().__init__(priors=priors, var_smoothing=var_smoothing)\n",
    "        self.tfidf_max_features = tfidf_max_features\n",
    "        self.ngram_range = ngram_range\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        self.tfidf = mtfidf.myTFIDF(X, self.tfidf_max_features, self.ngram_range)\n",
    "        X = self.tfidf.df_tfidf_vectorize(X).todense()\n",
    "        return super().fit(X, y, sample_weight=sample_weight)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #tfidf = mtfidf.myTFIDF(self.tfidf_max_features, self.ngram_range)\n",
    "        X = self.tfidf.df_tfidf_vectorize(X).todense()\n",
    "        return super().predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5617e",
   "metadata": {},
   "source": [
    "And here it is the implementation of the Gaussian Naive Bayes with Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae8f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2VGaussianNB(GaussianNB):\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 vector_size=100,\n",
    "                 window=5,\n",
    "                 min_count=1,\n",
    "                 n_proc=15,\n",
    "                 epochs=100,\n",
    "                 priors=None,\n",
    "                 var_smoothing=1e-9):\n",
    "        super().__init__(priors=priors, var_smoothing=var_smoothing)\n",
    "        self.vector_size=vector_size\n",
    "        self.window=window\n",
    "        self.min_count=min_count\n",
    "        self.n_proc=n_proc\n",
    "        self.epochs=epochs\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        self.W2V = w2v.myWord2Vec(corpus=X,\n",
    "                                  vector_size=self.vector_size,\n",
    "                                  window=self.window,\n",
    "                                  min_count=self.min_count,\n",
    "                                  n_proc=self.n_proc,\n",
    "                                  epochs=self.epochs)\n",
    "        X = self.W2V.df_w2v_vectorize(X)\n",
    "        \n",
    "        return super().fit(X, y, sample_weight=sample_weight)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.W2V.df_w2v_vectorize(X)\n",
    "        return super().predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e17cde2",
   "metadata": {},
   "source": [
    "###### Multinomial Naive Bayes\n",
    "\n",
    "Feature vectors represent the frequencies with which certain events have been generated by a multinomial distribution. This is the event model typically used for document classification.\n",
    "\n",
    "Here it is the implementation of the Multinomial Naive Bayes with TFIDF embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a0b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFMultinomialNB(MultinomialNB):\n",
    "    def __init__(self, *, tfidf_max_features=100, ngram_range=(1, 2), alpha=1.0, fit_prior=True, class_prior=None):\n",
    "        super().__init__(alpha=alpha, fit_prior=fit_prior, class_prior=class_prior)\n",
    "        self.tfidf_max_features = tfidf_max_features\n",
    "        self.ngram_range = ngram_range\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        self.tfidf = mtfidf.myTFIDF(X, self.tfidf_max_features, self.ngram_range)\n",
    "        X = self.tfidf.df_tfidf_vectorize(X)\n",
    "        return super().fit(X, y, sample_weight=sample_weight)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.tfidf.df_tfidf_vectorize(X)\n",
    "        return super().predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db08f93c",
   "metadata": {},
   "source": [
    "And here it is the implementation of the Multinomial Naive Bayes with word2vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8cdc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2VMultinomialNB(MultinomialNB):\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 vector_size=100,\n",
    "                 window=5,\n",
    "                 min_count=1,\n",
    "                 n_proc=15,\n",
    "                 epochs=100,\n",
    "                 alpha=1.0,\n",
    "                 fit_prior=True,\n",
    "                 class_prior=None):\n",
    "        super().__init__(alpha=alpha, fit_prior=fit_prior, class_prior=class_prior)\n",
    "        self.vector_size=vector_size\n",
    "        self.window=window\n",
    "        self.min_count=min_count\n",
    "        self.n_proc=n_proc\n",
    "        self.epochs=epochs\n",
    "        self.scaler = MinMaxScaler()\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        self.W2V = w2v.myWord2Vec(corpus=X,\n",
    "                                  vector_size=self.vector_size,\n",
    "                                  window=self.window,\n",
    "                                  min_count=self.min_count,\n",
    "                                  n_proc=self.n_proc,\n",
    "                                  epochs=self.epochs)\n",
    "        X = self.W2V.df_w2v_vectorize(X)\n",
    "        self.scaler.fit(X)\n",
    "        X = self.scaler.transform(X)\n",
    "        return super().fit(X, y, sample_weight=sample_weight)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.W2V.df_w2v_vectorize(X)\n",
    "        X = self.scaler.transform(X)\n",
    "        return super().predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef50860",
   "metadata": {},
   "source": [
    "###### Bernoulli Naive Bayes\n",
    "\n",
    "In the multivariate Bernoulli event model, features are independent booleans (binary variables) describing inputs. Like the multinomial model, this model is popular for document classification tasks, where binary term occurrence(i.e. a word occurs in a document or not) features are used rather than term frequencies(i.e. frequency of a word in the document).\n",
    "\n",
    "This model is then not consistent with our case and then it will not be used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
